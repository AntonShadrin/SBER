{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "\n",
    "# data.mail data\n",
    "# data_train = pd.read_csv('products_sentiment_train.tsv',sep='\\t',header=None)\n",
    "# data_test = pd.read_csv('products_sentiment_test.tsv',sep='\\t')\n",
    "\n",
    "# AMAZON data\n",
    "data_train = pd.read_csv('train.ft.txt',sep='\\t',header=None, nrows=10000)\n",
    "data_test = pd.read_csv('products_sentiment_train.tsv',sep='\\t',header=None)\n",
    "\n",
    "# data_train[0] = data_train[0].map(lambda x: re.sub('[^a-zA-Z ]','', x.lower()))\n",
    "# data_train['len'] = data_train[0].map(lambda x: len(x))\n",
    "# data_test['text'] = data_test['text'].map(lambda x: re.sub('[^a-zA-Z ]','', x.lower()))\n",
    "# data_test['len'] = data_test['text'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0          2 . take around 10,000 640x480 pictures .      1\n",
      "1  i downloaded a trial version of computer assoc...      1\n",
      "2  the wrt54g plus the hga7t is a perfect solutio...      1\n",
      "3  i dont especially like how music files are uns...      0\n",
      "4  i was using the cheapie pail ... and it worked...      1\n"
     ]
    }
   ],
   "source": [
    "# preprocessing data\n",
    "\n",
    "data_train['label'] = data_train[0].apply(lambda x: int(x[9]))\n",
    "# rename\n",
    "data_train.rename(columns={0:'text'},inplace=True)\n",
    "data_test.rename(columns={0:'text',1:'label'},inplace=True)\n",
    "\n",
    "data_train['text'] = data_train['text'].apply(lambda x: x[11:].lower())\n",
    "data_test['text'] = data_test['text'].apply(lambda x: x.lower())\n",
    "data_train.loc[data_train['label']==1,'label']=0\n",
    "data_train.loc[data_train['label']==2,'label']=1\n",
    "print(data_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.2760153003384427\n",
      "6.2105263157894735 0.25460503755539526\n",
      "11.421052631578947 0.25989785948337196\n",
      "16.63157894736842 0.2667963624558804\n",
      "21.842105263157894 0.273750614919507\n",
      "27.052631578947366 0.2804565417444491\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-a934afdb5972>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlog_los_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1548\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1549\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    919\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    922\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# testing logistic_regression\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(data_train['text'])\n",
    "y = data_train['label'].values\n",
    "CV = KFold(n_splits=5,shuffle=True,random_state=241)\n",
    "# np.power(10,np.linspace(-5,5,11))\n",
    "# np.linspace(0.01,10,20)\n",
    "for C in np.linspace(1,10,5):\n",
    "    log_los_values = []\n",
    "    for train_index, test_index in CV.split(X):\n",
    "        lr = LogisticRegression(penalty='l2',C=C)\n",
    "        lr.fit(X[train_index],y[train_index])\n",
    "        predict = lr.predict_proba(X[test_index])\n",
    "        log_los_values.append(log_loss(y[test_index],predict[:,1]))\n",
    "        #print(cv_results['test_score'])\n",
    "    print(C, np.mean(log_los_values))\n",
    "# 2.511886431509581 0.4820568735263769\n",
    "# 2.323232323232323 0.48158033118945165\n",
    "# 5.474747474747475 0.46380617530954443\n",
    "# 5.451451451451451 0.463805939803994\n",
    "# 5.4535353535353535 0.4638059371066535\n",
    "# 5.929292929292929 0.4621232668547865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5390215732075905"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create result logistic_regression\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_train = tfidf_vec.fit_transform(data_train['text'])\n",
    "y_train = data_train['label'].values\n",
    "lr = LogisticRegression(penalty='l2',C=5.4535353535353535)\n",
    "lr.fit(X_train,y_train)\n",
    "X_test = tfidf_vec.transform(data_test['text'])\n",
    "predict = lr.predict_proba(X_test)\n",
    "\n",
    "log_loss(data_test['label'],predict[:,1])\n",
    "\n",
    "# data_test['y'] = predict[:,1]\n",
    "# data_test.drop(columns='text',inplace=True)\n",
    "# data_test.to_csv('result_lr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create result\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_train = tfidf_vec.fit_transform(data_train[0])\n",
    "# print(X_train[:10,...])\n",
    "y_train = data_train[1].values\n",
    "lr = LogisticRegression(penalty='l2',C=5.4535353535353535)\n",
    "lr.fit(X_train,y_train)\n",
    "X_test = tfidf_vec.transform(data_test['text'])\n",
    "predict = lr.predict_proba(X_test)\n",
    "data_test['y'] = predict[:,1]\n",
    "data_test.drop(columns='text',inplace=True)\n",
    "data_test.to_csv('result_lr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.358107657183621\n",
      "0.6444444444444445 0.33464764547293524\n"
     ]
    }
   ],
   "source": [
    "# testing SVC\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(data_train['text'])\n",
    "y = data_train['label'].values\n",
    "CV = KFold(n_splits=5,shuffle=True,random_state=241)\n",
    "# np.power(10,np.linspace(-5,5,11))\n",
    "# np.linspace(0.01,10,20)\n",
    "for gamma in np.linspace(0.1,5,10):\n",
    "    log_los_values = []\n",
    "    for train_index, test_index in CV.split(X):\n",
    "        svc = svm.SVC(gamma=gamma,probability=True)\n",
    "        svc.fit(X[train_index],y[train_index])\n",
    "        predict = svc.predict_proba(X[test_index])\n",
    "    log_los_values.append(log_loss(y[test_index],predict[:,1]))\n",
    "    print(gamma, np.mean(log_los_values))\n",
    "\n",
    "\n",
    "# tfidf_vec = TfidfVectorizer()\n",
    "# X = tfidf_vec.fit_transform(data_train[0])\n",
    "# y = data_train[1].values\n",
    "# CV = KFold(n_splits=5,shuffle=True,random_state=241)\n",
    "# # np.power(10,np.linspace(0,1,100))\n",
    "# # np.linspace(0.01,10,20)\n",
    "# # for ker in ['poly', 'rbf', 'sigmoid']:\n",
    "# for ker in ['rbf']:\n",
    "#     for gamma in np.linspace(0.1,5,20):\n",
    "#         log_los_values = []\n",
    "#         for train_index, test_index in CV.split(X):\n",
    "#             svc = svm.SVC(kernel=ker,gamma=gamma,probability=True)\n",
    "#             svc.fit(X[train_index],y[train_index])\n",
    "#             predict = svc.predict_proba(X[test_index])\n",
    "#     #         print(predict.shape)\n",
    "#             log_los_values.append(log_loss(y[test_index],predict))\n",
    "#         print(f'kernal={ker}',gamma, np.mean(log_los_values))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05494269 0.94505731]\n",
      " [0.73234271 0.26765729]\n",
      " [0.10999111 0.89000889]\n",
      " [0.29136253 0.70863747]\n",
      " [0.62902439 0.37097561]\n",
      " [0.46072725 0.53927275]\n",
      " [0.07783871 0.92216129]\n",
      " [0.74498778 0.25501222]\n",
      " [0.89255639 0.10744361]\n",
      " [0.0402188  0.9597812 ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(data_train[0])\n",
    "y = data_train[1].values\n",
    "lr = LogisticRegression(penalty='l2',C=5.4535353535353535)\n",
    "lr.fit(X,y)\n",
    "X_test = tfidf_vec.transform(data_test['text'])\n",
    "predict = lr.predict_proba(X_test)\n",
    "print(predict[:10])\n",
    "data_test['y'] = predict[:,1]\n",
    "data_test.drop(columns='text',inplace=True)\n",
    "data_test.to_csv('result_lr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  (0, 3387)\t1\n",
      "  (0, 182)\t1\n",
      "  (0, 2469)\t1\n",
      "  (1, 972)\t1\n",
      "  (1, 3547)\t1\n",
      "  (1, 3685)\t1\n",
      "  (1, 2274)\t1\n",
      "  (1, 637)\t2\n",
      "  (1, 197)\t1\n",
      "  (1, 1167)\t1\n",
      "  (1, 1241)\t1\n",
      "  (1, 129)\t2\n",
      "  (1, 141)\t1\n",
      "  (1, 1216)\t1\n",
      "  (1, 1629)\t1\n",
      "  (1, 1935)\t1\n",
      "  (1, 3834)\t1\n",
      "  (1, 2983)\t1\n",
      "  (1, 3376)\t1\n",
      "  (1, 99)\t1\n",
      "  (1, 2356)\t1\n",
      "  (1, 80)\t1\n",
      "  (2, 1629)\t1\n",
      "  (2, 3439)\t2\n",
      "  (2, 3880)\t1\n",
      "  :\t:\n",
      "  (8, 86)\t1\n",
      "  (8, 3166)\t1\n",
      "  (8, 2243)\t1\n",
      "  (8, 1510)\t1\n",
      "  (8, 1074)\t1\n",
      "  (8, 3331)\t1\n",
      "  (8, 142)\t1\n",
      "  (8, 2106)\t1\n",
      "  (9, 3439)\t2\n",
      "  (9, 1719)\t1\n",
      "  (9, 3894)\t1\n",
      "  (9, 2325)\t1\n",
      "  (9, 176)\t1\n",
      "  (9, 3663)\t1\n",
      "  (9, 2385)\t1\n",
      "  (9, 3488)\t1\n",
      "  (9, 3458)\t1\n",
      "  (9, 3796)\t1\n",
      "  (9, 932)\t1\n",
      "  (9, 567)\t1\n",
      "  (9, 885)\t1\n",
      "  (9, 3557)\t1\n",
      "  (9, 2300)\t1\n",
      "  (9, 884)\t1\n",
      "  (9, 1382)\t1\n",
      "1e-05 0.6931471805599452\n",
      "0.0001 0.6931471805599452\n",
      "0.001 0.6931471805599452\n",
      "0.01 0.6430126084890624\n",
      "0.1 0.5208610732583445\n",
      "1.0 0.8359718383331719\n",
      "10.0 1.2614677996276922\n",
      "100.0 1.4919501610812391\n",
      "1000.0 1.6880817863297062\n",
      "10000.0 1.8880640959562336\n",
      "100000.0 1.960423630978972\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create result all\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "X_train = data_train[0].values[:,np.newaxis]\n",
    "train_size = X_train.shape[0]\n",
    "X_test = data_test['text'].values[:,np.newaxis]\n",
    "X_all = np.vstack((X_train,X_test))\n",
    "X_all = pd.DataFrame(X_all)\n",
    "\n",
    "X_all = tfidf_vec.fit_transform(X_all[0])\n",
    "X_train = X_all[:train_size]\n",
    "X_test = X_all[train_size:]\n",
    "# print(X_train[:10,...])\n",
    "y_train = data_train[1].values\n",
    "lr = LogisticRegression(penalty='l2',C=5.516326530612245)\n",
    "lr.fit(X_train,y_train)\n",
    "predict = lr.predict_proba(X_test)\n",
    "data_test['y'] = predict[:,1]\n",
    "data_test.drop(columns='text',inplace=True)\n",
    "data_test[['Id','y']].to_csv('result_lr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(2000, 4424) (2000,)\n",
      "5.2 0.46336824282921363\n",
      "5.210204081632654 0.4633645206966658\n",
      "5.220408163265306 0.4633609832731086\n",
      "5.2306122448979595 0.463357574385778\n",
      "5.240816326530612 0.4633542932277409\n",
      "5.251020408163265 0.4633511391025067\n",
      "5.261224489795919 0.46334811149320243\n",
      "5.271428571428571 0.46334520963785036\n",
      "5.281632653061225 0.46334243266474295\n",
      "5.291836734693877 0.46333978010136373\n",
      "5.302040816326531 0.4633372511378565\n",
      "5.312244897959184 0.4633348452872946\n",
      "5.322448979591837 0.46333256165060793\n",
      "5.33265306122449 0.4633303997358197\n",
      "5.342857142857143 0.4633283587920607\n",
      "5.353061224489796 0.4633264380752147\n",
      "5.363265306122449 0.4633246372065664\n",
      "5.373469387755102 0.46332295536783413\n",
      "5.383673469387755 0.4633213918623019\n",
      "5.393877551020409 0.4633199461480234\n",
      "5.404081632653061 0.463318617620342\n",
      "5.414285714285715 0.4633174055781262\n",
      "5.424489795918367 0.4633163093249245\n",
      "5.4346938775510205 0.46331468386520047\n",
      "5.444897959183674 0.4633138136450429\n",
      "5.455102040816326 0.46331305744635837\n",
      "5.46530612244898 0.4633124146240427\n",
      "5.475510204081633 0.4633118845740074\n",
      "5.485714285714286 0.4633114667304519\n",
      "5.495918367346939 0.4633111604658737\n",
      "5.506122448979592 0.4633109651393828\n",
      "5.516326530612245 0.4633108802416294\n",
      "5.5265306122448985 0.463310905149929\n",
      "5.536734693877551 0.4633110393071621\n",
      "5.546938775510204 0.46331128208597167\n",
      "5.557142857142857 0.46331163300195166\n",
      "5.56734693877551 0.46331209135299184\n",
      "5.577551020408164 0.4633126566783309\n",
      "5.587755102040816 0.46331332834069344\n",
      "5.59795918367347 0.4633141058495783\n",
      "5.608163265306122 0.46331498861724496\n",
      "5.618367346938776 0.4633159760682576\n",
      "5.628571428571429 0.46331706764370634\n",
      "5.6387755102040815 0.4633182629263408\n",
      "5.648979591836735 0.46331956121752577\n",
      "5.659183673469387 0.4633209620092624\n",
      "5.669387755102041 0.4633224648050752\n",
      "5.679591836734694 0.4633240691033821\n",
      "5.689795918367347 0.46332577427661104\n",
      "5.7 0.46332757986804224\n"
     ]
    }
   ],
   "source": [
    "# testing all\n",
    "print('\\n')\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_train = data_train[0].values[:,np.newaxis]\n",
    "train_size = X_train.shape[0]\n",
    "X_test = data_test['text'].values[:,np.newaxis]\n",
    "X_all = np.vstack((X_train,X_test))\n",
    "X_all = pd.DataFrame(X_all)\n",
    "X_all = tfidf_vec.fit_transform(X_all[0])\n",
    "X = X_all[:train_size]\n",
    "\n",
    "y = data_train[1].values\n",
    "print(X.shape,y.shape)\n",
    "CV = KFold(n_splits=5,shuffle=True,random_state=241)\n",
    "# np.power(10,np.linspace(-5,5,11))\n",
    "# np.linspace(0.1,100,20)\n",
    "for C in np.linspace(5.2,5.7,50):\n",
    "    log_los_values = []\n",
    "    for train_index, test_index in CV.split(X):\n",
    "        lr = LogisticRegression(penalty='l2',C=C)\n",
    "        lr.fit(X[train_index],y[train_index])\n",
    "        predict = lr.predict_proba(X[test_index])\n",
    "        log_los_values.append(log_loss(y[test_index],predict[:,1]))\n",
    "        #print(cv_results['test_score'])\n",
    "    print(C, np.mean(log_los_values))\n",
    "# 2.511886431509581 0.4820568735263769\n",
    "# 2.323232323232323 0.48158033118945165\n",
    "# 5.474747474747475 0.46380617530954443\n",
    "# 5.451451451451451 0.463805939803994\n",
    "# 5.4535353535353535 0.4638059371066535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 learning_rate=0.12 0.5247289270669494\n",
      "6 learning_rate=0.1 0.5222220719833285\n",
      "6 learning_rate=0.08 0.5290117317831526\n",
      "6 learning_rate=0.06 0.5308788618672785\n"
     ]
    }
   ],
   "source": [
    "# testing boosting\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(data_train[0])\n",
    "y = data_train[1].values\n",
    "\n",
    "# X = pd.DataFrame(X)\n",
    "# print(X.head())\n",
    "\n",
    "CV = KFold(n_splits=5,shuffle=True,random_state=241)\n",
    "# np.power(10,np.linspace(0,1,100))\n",
    "# np.linspace(0.01,10,20)\n",
    "# for n_estimators in [150]:\n",
    "for max_depth in [6]:\n",
    "    for learning_rate  in [0.12, 0.1, 0.08, 0.06]:\n",
    "        log_los_values = []\n",
    "        for train_index, test_index in CV.split(X):\n",
    "            gbc = GradientBoostingClassifier(learning_rate=learning_rate, max_depth=max_depth)\n",
    "            gbc.fit(X[train_index],y[train_index])\n",
    "            predict = gbc.predict_proba(X[test_index])\n",
    "            log_los_values.append(log_loss(y[test_index],predict[:,1]))\n",
    "        print(max_depth, f'learning_rate={learning_rate}', np.mean(log_los_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 learning_rate=0.2 0.5240379282728987\n",
      "4 learning_rate=0.15 0.5213225539641342\n",
      "4 learning_rate=0.1 0.5244499643281955\n",
      "4 learning_rate=0.05 0.53806197522738\n",
      "5 learning_rate=0.2 0.5298959235282794\n",
      "5 learning_rate=0.15 0.5228579214573299\n",
      "5 learning_rate=0.1 0.5219616173499515\n",
      "5 learning_rate=0.05 0.5319201163167518\n",
      "6 learning_rate=0.2 0.5455012410005685\n",
      "6 learning_rate=0.15 0.531285834827399\n",
      "6 learning_rate=0.1 0.5183754542228789\n",
      "6 learning_rate=0.05 0.5265408237213677\n"
     ]
    }
   ],
   "source": [
    "# test boosting all\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_train = data_train[0].values[:,np.newaxis]\n",
    "train_size = X_train.shape[0]\n",
    "X_test = data_test['text'].values[:,np.newaxis]\n",
    "X_all = np.vstack((X_train,X_test))\n",
    "X_all = pd.DataFrame(X_all)\n",
    "X_all = tfidf_vec.fit_transform(X_all[0])\n",
    "X = X_all[:train_size]\n",
    "y = data_train[1].values\n",
    "\n",
    "CV = KFold(n_splits=5,shuffle=True,random_state=241)\n",
    "# np.power(10,np.linspace(0,1,100))\n",
    "# np.linspace(0.01,10,20)\n",
    "# for n_estimators in [150]:\n",
    "for max_depth in [4,5,6]:\n",
    "    for learning_rate  in [0.2, 0.15, 0.1, 0.05]:\n",
    "        log_los_values = []\n",
    "        for train_index, test_index in CV.split(X):\n",
    "            gbc = GradientBoostingClassifier(n_estimators=150, learning_rate=learning_rate, max_depth=max_depth)\n",
    "            gbc.fit(X[train_index],y[train_index])\n",
    "            predict = gbc.predict_proba(X[test_index])\n",
    "            log_los_values.append(log_loss(y[test_index],predict[:,1]))\n",
    "        print(max_depth, f'learning_rate={learning_rate}', np.mean(log_los_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "my_const=0.7 max_depth=3 log_loss=0.46209454434653663\n",
      "my_const=0.72 max_depth=3 log_loss=0.46235528131905035\n",
      "my_const=0.74 max_depth=3 log_loss=0.46187921995275183\n",
      "my_const=0.76 max_depth=3 log_loss=0.4615832587575611\n",
      "my_const=0.78 max_depth=3 log_loss=0.46139022113954276\n",
      "my_const=0.8 max_depth=3 log_loss=0.46167967788689995\n"
     ]
    }
   ],
   "source": [
    "# test lr & gbm\n",
    "print('\\n')\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(data_train[0])\n",
    "y = data_train[1].values\n",
    "CV = KFold(n_splits=5,shuffle=True,random_state=241)\n",
    "# np.power(10,np.linspace(0,1,100))\n",
    "# np.linspace(0.01,10,20)\n",
    "\n",
    "\n",
    "for my_const in [0.7,0.72,0.74,0.76,0.78,0.8]:\n",
    "    for max_depth in [3,]:\n",
    "        for learning_rate  in [0.1]:\n",
    "            log_los_values = []\n",
    "            for train_index, test_index in CV.split(X):\n",
    "                lr = LogisticRegression(penalty='l2',C=5.4535353535353535)\n",
    "                gbc = GradientBoostingClassifier(learning_rate=learning_rate, max_depth=max_depth)\n",
    "                lr.fit(X[train_index],y[train_index])\n",
    "                predict_lr = lr.predict_proba(X[test_index])[:,1]\n",
    "                gbc.fit(X[train_index],y[train_index])\n",
    "                predict_gbc = gbc.predict_proba(X[test_index])[:,1]\n",
    "                predict = []\n",
    "                for i in np.arange(len(predict_lr)):\n",
    "                    if (predict_lr[i]>my_const) and (predict_gbc[i]>my_const):\n",
    "                        predict.append(np.max([predict_lr[i],predict_gbc[i]]))\n",
    "#                         predict.append(1)\n",
    "                    else:\n",
    "                        if (predict_lr[i]<1.0-my_const) and (predict_gbc[i]<1.0-my_const):\n",
    "#                             predict.append(0)\n",
    "                            predict.append(np.min([predict_lr[i],predict_gbc[i]]))\n",
    "                        else:\n",
    "                            predict.append(predict_lr[i]) \n",
    "                log_los_values.append(log_loss(y[test_index],predict))\n",
    "                #print(cv_results['test_score'])\n",
    "            print(f'my_const={my_const}',f'max_depth={max_depth}', f'log_loss={np.mean(log_los_values)}')\n",
    "# my_const=0.78 max_depth=3 log_loss=0.46139022113954276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create result lr & gbm\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_train = tfidf_vec.fit_transform(data_train[0])\n",
    "# print(X_train[:10,...])\n",
    "y_train = data_train[1].values\n",
    "X_test = tfidf_vec.transform(data_test['text'])\n",
    "my_const=0.78\n",
    "\n",
    "lr = LogisticRegression(penalty='l2',C=5.4535353535353535)\n",
    "gbc = GradientBoostingClassifier()\n",
    "lr.fit(X_train,y_train)\n",
    "predict_lr = lr.predict_proba(X_test)[:,1]\n",
    "gbc.fit(X_train,y_train)\n",
    "predict_gbc = gbc.predict_proba(X_test)[:,1]\n",
    "predict = []\n",
    "for i in np.arange(len(predict_lr)):\n",
    "    if (predict_lr[i]>my_const) and (predict_gbc[i]>my_const):\n",
    "        predict.append(np.max([predict_lr[i],predict_gbc[i]]))\n",
    "    else:\n",
    "        if (predict_lr[i]<1.0-my_const) and (predict_gbc[i]<1.0-my_const):\n",
    "            predict.append(np.min([predict_lr[i],predict_gbc[i]]))\n",
    "        else:\n",
    "            predict.append(predict_lr[i]) \n",
    "\n",
    "\n",
    "\n",
    "data_test['y'] = predict\n",
    "data_test.drop(columns='text',inplace=True)\n",
    "data_test.to_csv('result_lr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "kernal=rbf 0.1 0.47445741414027703\n",
      "kernal=rbf 0.35789473684210527 0.4635631714270363\n",
      "kernal=rbf 0.6157894736842106 0.46046140865244667\n",
      "kernal=rbf 0.8736842105263158 0.45726590536156264\n",
      "kernal=rbf 1.1315789473684212 0.4551298563466172\n",
      "kernal=rbf 1.3894736842105266 0.45317260284083155\n",
      "kernal=rbf 1.6473684210526318 0.45204670107772715\n",
      "kernal=rbf 1.905263157894737 0.4520810842863046\n",
      "kernal=rbf 2.1631578947368424 0.4526473661370642\n",
      "kernal=rbf 2.421052631578948 0.45228601281545233\n",
      "kernal=rbf 2.678947368421053 0.45429239768373675\n",
      "kernal=rbf 2.936842105263158 0.4538788448852033\n",
      "kernal=rbf 3.1947368421052635 0.45262386146546774\n",
      "kernal=rbf 3.452631578947369 0.45320128481283517\n",
      "kernal=rbf 3.710526315789474 0.4724836735693308\n",
      "kernal=rbf 3.9684210526315793 0.4875705515449775\n",
      "kernal=rbf 4.226315789473684 0.5159940410379049\n",
      "kernal=rbf 4.484210526315789 0.5769910492059271\n",
      "kernal=rbf 4.742105263157895 0.5934057152978102\n",
      "kernal=rbf 5.0 0.6281182421315485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing SVC\n",
    "print('\\n')\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(data_train[0])\n",
    "y = data_train[1].values\n",
    "CV = KFold(n_splits=5,shuffle=True,random_state=241)\n",
    "# np.power(10,np.linspace(0,1,100))\n",
    "# np.linspace(0.01,10,20)\n",
    "# for ker in ['poly', 'rbf', 'sigmoid']:\n",
    "for ker in ['rbf']:\n",
    "    for gamma in np.linspace(0.1,5,20):\n",
    "        log_los_values = []\n",
    "        for train_index, test_index in CV.split(X):\n",
    "            svc = svm.SVC(kernel=ker,gamma=gamma,probability=True)\n",
    "            svc.fit(X[train_index],y[train_index])\n",
    "            predict = svc.predict_proba(X[test_index])\n",
    "    #         print(predict.shape)\n",
    "            log_los_values.append(log_loss(y[test_index],predict))\n",
    "        print(f'kernal={ker}',gamma, np.mean(log_los_values))\n",
    "    print()\n",
    "\n",
    "# kernal=rbf 1.7473684210526315 0.452176475069442\n",
    "# kernal=rbf 1.9684210526315788 0.4520421615276179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.1 0.5946426076814832\n",
      "C=0.2 0.5885814471468649\n",
      "C=0.30000000000000004 0.5882908917452914\n",
      "C=0.4 0.5882718082307916\n",
      "C=0.5 0.5882226871707538\n",
      "C=0.6 0.5885091784490525\n",
      "C=0.7000000000000001 0.5887329831778425\n",
      "C=0.8 0.5894953627531114\n",
      "C=0.9 0.5875953783347192\n",
      "C=1.0 0.5861250179468526\n",
      "C=1.1 0.5850241370309297\n",
      "C=1.2000000000000002 0.584115313559634\n",
      "C=1.3000000000000003 0.5833249992930905\n",
      "C=1.4000000000000001 0.5826414467822387\n",
      "C=1.5000000000000002 0.581980407461795\n",
      "C=1.6 0.5813965004247106\n",
      "C=1.7000000000000002 0.5808863817392089\n",
      "C=1.8000000000000003 0.5804920821301753\n",
      "C=1.9000000000000001 0.5801652903260501\n",
      "C=2.0 0.5798514078202511\n",
      "C=2.1 0.5795555463389299\n",
      "C=2.2 0.5792894608626362\n",
      "C=2.3000000000000003 0.5790628014477215\n",
      "C=2.4000000000000004 0.5788595342946696\n",
      "C=2.5000000000000004 0.5786675564902478\n",
      "C=2.6 0.5784947039425343\n",
      "C=2.7 0.5783242251090924\n",
      "C=2.8000000000000003 0.5781527825713699\n",
      "C=2.9000000000000004 0.577971588034194\n",
      "C=3.0000000000000004 0.5777242036061592\n",
      "C=3.1 0.5776695126429112\n",
      "C=3.2 0.5776634993204409\n",
      "C=3.3000000000000003 0.5776788143261822\n",
      "C=3.4000000000000004 0.5776923641443339\n",
      "C=3.5000000000000004 0.5777103136188504\n",
      "C=3.6 0.577728091250077\n",
      "C=3.7 0.5777459255596666\n",
      "C=3.8000000000000003 0.5777645885209416\n",
      "C=3.9000000000000004 0.5777838419521607\n",
      "C=4.0 0.5778145791901628\n",
      "C=4.1 0.5778456517725846\n",
      "C=4.2 0.5778838979634714\n",
      "C=4.3 0.5779240469385271\n",
      "C=4.3999999999999995 0.5779612873944922\n",
      "C=4.5 0.5780004534727217\n",
      "C=4.6 0.5780396418795336\n",
      "C=4.7 0.5780786886520103\n",
      "C=4.8 0.5781152531449686\n",
      "C=4.9 0.5781535795131847\n",
      "C=5.0 0.5781909344453652\n",
      "C=5.1 0.5782278013877346\n",
      "C=5.2 0.5782659792766884\n",
      "C=5.3 0.5783103997573726\n",
      "C=5.4 0.5784089800789669\n",
      "C=5.5 0.578502145407641\n",
      "C=5.6 0.5785949639705197\n",
      "C=5.7 0.5786899039743596\n",
      "C=5.8 0.5787837419011492\n",
      "C=5.9 0.5788743093415054\n",
      "C=6.0 0.5789632380091857\n",
      "C=6.1 0.5790524324779925\n",
      "C=6.2 0.5791311711122935\n",
      "C=6.3 0.5792062564019027\n",
      "C=6.4 0.5793076195904228\n",
      "C=6.5 0.5794138075426772\n",
      "C=6.6 0.579517741903488\n",
      "C=6.7 0.5796189211090192\n",
      "C=6.8 0.5797193606830571\n",
      "C=6.9 0.5798208761406154\n",
      "C=7.0 0.5799229072823111\n",
      "C=7.1 0.5800227861480008\n",
      "C=7.2 0.5801209718747492\n",
      "C=7.3 0.5802175302990433\n",
      "C=7.4 0.5803123619457798\n",
      "C=7.5 0.5804056066501899\n",
      "C=7.6 0.5804969404847906\n",
      "C=7.7 0.5805859021906452\n",
      "C=7.8 0.5806731559665198\n",
      "C=7.9 0.5807596583344183\n",
      "C=8.0 0.5808454338693338\n",
      "C=8.1 0.5809303145148499\n",
      "C=8.2 0.58101347876864\n",
      "C=8.3 0.581093919956003\n",
      "C=8.4 0.5811712430283007\n",
      "C=8.5 0.5812500584254294\n",
      "C=8.6 0.5813286572669784\n",
      "C=8.7 0.5814077347867732\n",
      "C=8.8 0.5814866423978582\n",
      "C=8.9 0.5815643450767138\n",
      "C=9.0 0.5816406902637918\n",
      "C=9.1 0.5817159406582373\n",
      "C=9.2 0.5817911948961297\n",
      "C=9.3 0.5818684981913222\n",
      "C=9.4 0.581944200028809\n",
      "C=9.5 0.5820187278240803\n",
      "C=9.6 0.5820922028244805\n",
      "C=9.700000000000001 0.5821649992935252\n",
      "C=9.8 0.582236308766378\n",
      "C=9.9 0.5823071756844189\n",
      "C=10.0 0.5823770869071362\n"
     ]
    }
   ],
   "source": [
    "# testing SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(data_train[0])\n",
    "y = data_train[1].values\n",
    "CV = KFold(n_splits=5,shuffle=True,random_state=241)\n",
    "# np.power(10,np.linspace(0,1,100))\n",
    "# np.linspace(0.01,10,20)\n",
    "for C in np.linspace(0.1,10,100):\n",
    "    log_los_values = []\n",
    "    for train_index, test_index in CV.split(X):\n",
    "        svc = LinearSVC(C=C, max_iter=10000)\n",
    "        svc.fit(X[train_index],y[train_index])\n",
    "        predict = svc.decision_function(X[test_index])\n",
    "        predict = (predict - predict.min()) / (predict.max() - predict.min())\n",
    "        log_los_values.append(log_loss(y[test_index],predict))\n",
    "    print(f'C={C}',np.mean(log_los_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n"
     ]
    }
   ],
   "source": [
    "# create result SVC\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_train =tfidf_vec.fit_transform(data_train[0])\n",
    "# print(X_train[:10,...])\n",
    "y_train = data_train[1].values\n",
    "X_test = tfidf_vec.transform(data_test['text'])\n",
    "\n",
    "\n",
    "svc = svm.SVC(kernel='rbf',gamma=1.9684210526315788,probability=True)\n",
    "svc.fit(X_train,y_train)\n",
    "predict = svc.predict_proba(X_test)\n",
    "print(predict.shape)\n",
    "data_test['y'] = predict[:,1]\n",
    "data_test.drop(columns='text',inplace=True)\n",
    "data_test.to_csv('result_lr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "print('\\n')\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(data_train['text'])\n",
    "y = data_train[1].values\n",
    "CV = KFold(n_splits=5,shuffle=True,random_state=241)\n",
    "# np.power(10,np.linspace(0,1,100))\n",
    "# np.linspace(0.01,10,20)\n",
    "for C in np.linspace(0.1,20,100):\n",
    "    log_los_values = []\n",
    "    for train_index, test_index in CV.split(X):\n",
    "        lr = LogisticRegression(penalty='l2',C=C)\n",
    "        lr.fit(X[train_index],y[train_index])\n",
    "        predict = lr.predict_proba(X[test_index])\n",
    "        log_los_values.append(log_loss(y[test_index],predict[:,1]))\n",
    "        #print(cv_results['test_score'])\n",
    "    print(C, np.mean(log_los_values))\n",
    "# 2.511886431509581 0.4820568735263769\n",
    "# 2.323232323232323 0.48158033118945165\n",
    "# 5.474747474747475 0.46380617530954443\n",
    "# 5.451451451451451 0.463805939803994\n",
    "# 5.4535353535353535 0.4638059371066535\n",
    "# 5.929292929292929 0.4621232668547865"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
